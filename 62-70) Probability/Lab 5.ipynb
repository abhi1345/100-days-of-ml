{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy!\n",
    "\n",
    "v1.0 (2018 Spring) Tavor Baharav, Kaylee Burns, Gary Cheng, Sinho Chewi, Hemang Jangle, William Gan, Alvin Kao, Chen Meng, Vrettos Muolos, Kanaad Parvate, Ray Ramamurti\n",
    "\n",
    "v1.1 (2018 Fall) Raghav Anand, Kurtland Chua, Payam Delgosha, William Gan, Avishek Ghosh, Justin Hong, Nikunj Jain, Katie Kang, Adarsh Karnati, Eric Liu, Kanaad Parvate, Ray Ramamurti, Amay Saxena "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue>$\\mathcal{Q}$1. Huffman Codes.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ben Bitdiddle is an avid coinflipper. He and his friend Alice enjoy sending each other the results of their coinflipping escapades, but unfortunately, they have a very minimal data plan. In order to get around this, Ben decides to try and $\\textit{compress}$ the sequence of coinflips he wants to communicate to Alice before sending it. He settles on his favorite method, Huffman Coding. He solidifies his scheme as follows:\n",
    "1. Flip a coin with heads (1) bias $p$ and record its value $M$ times.\n",
    "2. Encode and send the sequence of $M$ coin flips as a binary string using a Huffman code based on the coin flip frequencies determined by the hash table probDict you will generate. He's not sure how many coin flips he wants to group together as a single encoding symbol, so he leaves that as a variable $n$ for now.\n",
    "\n",
    "Before attempting this section, brush up on (or learn for the first time) <a href=\"https://www.siggraph.org/education/materials/HyperGraph/video/mpeg/mpegfaq/huffman_tutorial.html\">Huffman coding</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <font color=blue>a. Implement a method generateProbabilities that, given $n,p$, outputs a dictionary mapping sequences of $n$ coinflips to their associated probabilities. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateProbabilities(p,n):\n",
    "    \"\"\"Return a dictionary (probDict) which maps all 2**n possible sequences of n coin flips to their\n",
    "        probability, given a heads (1) bias of p\n",
    "        generateProbabilities(.9,2) = {'00': .01, '01': .09, '10': .09, '11': .81}\"\"\"\n",
    "    \n",
    "    probDict = {}\n",
    "\n",
    "    ### Your code here\n",
    "\n",
    "    return probDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>b. Implement a method HuffEncode that, given a list of frequencies, will output the corresponding mapping of input symbol to Huffman codewords. Write a subsequent method encode_string that encodes a string given $n$ and the huffman dictionary.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports: heapq might be useful\n",
    "\n",
    "def HuffEncode(freq_dict):\n",
    "    \"\"\"Return a dictionary (flips2huff) which maps keys from the input dictionary freq_dict\n",
    "       to bitstrings using a Huffman code based on the frequencies of each key\"\"\"\n",
    "\n",
    "    \n",
    "    # Your Beautiful Code Here\n",
    "    return {}\n",
    "        \n",
    "\n",
    "def encode_string(string, flip2huff,n):\n",
    "    \"\"\"Return a bitstring encoded according to the Huffman code defined in the dictionary flip2huff.\n",
    "    We assume n divides the length of string\"\"\"\n",
    "    \n",
    "    # Your Beautiful Code Here    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "entropy = lambda x : -x*np.log2(x) - (1-x)*np.log2(1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>c. Plot Generation</font>\n",
    "Ben isn't sure what value of $n$ to pick, so he decides to test his compression scheme using different values of $n$.\n",
    "\n",
    "Using the functions you wrote above, lets run some simulations! In order to find the best $n$, plot $n$ on your x axis, and fraction of bits we need to use $\\left( \\frac{\\text{Compressed Length}}{\\text{Uncompressed length}} \\right)$ on the y axis. For each setting, average over 100 trials to reduce noise. Generate plots for p = .5,.75,.97 (3 total plots). For each plot, use:\n",
    "\n",
    "$n = 1,2,...,15$\n",
    "\n",
    "$M \\approx 1000$ (this is to avoid truncation errors, e.g. for n=3, use 1002).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your beautiful simulation code here\n",
    "\n",
    "p_list = [0.5, 0.75, .97] #coin '1' bias\n",
    "nVals = range(1,16) #encode n coin flips\n",
    "\n",
    "numFlips = 1000\n",
    "numTrials = 100\n",
    "\n",
    "averageCompression = []\n",
    "for i in range(len(p_list)):\n",
    "    p = p_list[i]\n",
    "    for n in nVals:\n",
    "        probDict = generateProbabilities(p,n)\n",
    "\n",
    "        flip2huff = HuffEncode(probDict)\n",
    "\n",
    "        numFlipsP = ((numFlips-1)//n + 1) * n # to prevent truncation in the encoding\n",
    "        for _ in range(numTrials):\n",
    "            ### your code here\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the three graphs here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ben shows this graph to Alice, surprised that his compression ratio keeps improving as he increases $n$, and seems to be asymptoting. Alice tells him of course, and that there exists an information theoretic lower bound.\n",
    "### <font color=blue>d. Find the relevant information theoretic lower bound, and add it as a horizontal line to your 3 plots above.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Wow, this is great!\" Ben exclaims. He suggests continuing to increase $n$, to keep improving the compression ratio. Alice tells him that there's a serious problem with this.\n",
    "\n",
    "### <font color=blue>e. What issue arises as $n$ becomes large?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue>$\\mathcal{Q}$2. Typical Sets.</font>\n",
    "We will now explore the notion of $\\textit{Typical Sets}$, as covered in the homework. This will help solidify your understanding of entropy, and your understanding of Shannon's theorem. As you recall from the homework, $\\textit{Typical Sets}$ includes all the events with a probability within the range of ($2^{-n(H(p) + \\epsilon)}$, $2^{-n(H(p) - \\epsilon)}$).\n",
    "\n",
    "### <font color = blue> a. Plotting</font>\n",
    "For $p=.6$, $n=10,...,500$, determine which elements would appear in the typical set $A_\\epsilon^{(n)}$, for $\\epsilon = .02$. Generate 3 plots with $n$ on the x axis, one with the probability of the typical set $P(A_\\epsilon^{(n)})$ on the y axis, another with $\\frac{1}{n} \\log_2 |A_\\epsilon^{(n)}|$, and a third with the fraction of events in the typical set $\\frac{|A_\\epsilon^{(n)}|}{2^n}$. (Note you don't have to enumerate all possible sequences - there are $2^{500}$. We are only asking for quantities related the probability of a sequence being in the typical set $P(A_\\epsilon^{(n)})$ or its size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your computation / plotting code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One way of thinking about the typical set asymptotically is that our compression function simply indexes each element in the typical set, numbering them $1,2,...,2^{nH(p)}$ ($nH(p)$ bits). All sequences outside of this typical set, we leave encoded as they are (n bits). If we look at the expected number of bits required to represent an symbol drawn according to the underlying distribution, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbb{E} [\\text{len}(x)] \n",
    "&= P(x \\in A_\\epsilon^{(n)}) \\cdot \\mathbb{E} [\\text{len}(x) | x \\in A_\\epsilon^{(n)}] + P(x \\notin A_\\epsilon^{(n)}) \\cdot \\mathbb{E} [\\text{len}(x) | x \\notin A_\\epsilon^{(n)}]\\\\\n",
    "&= P(x \\in A_\\epsilon^{(n)}) \\cdot nH(p) + P(x \\notin A_\\epsilon^{(n)}) \\cdot n\\\\\n",
    "& \\hspace{-.2cm} \\overset{n \\rightarrow \\infty}{=} 1 \\cdot n H(p) + 0 \\cdot n\\\\\n",
    "&= nH(p)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> b. Observations</font>\n",
    "\n",
    "Describe the asymptotic behavior of your 3 graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3) Entropy and Information Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous questions, we saw entropy being used as a limit for the extent we can compress a source of data. Now, we will explore an alternative interpretation of entropy as the amount of information contained in a random source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the following problem; we have 8 bins, numbered 1 through 8. There is a prize in exactly one of the bins, and each bin is equally likely to contain the prize. We'd like to figure out what which bin contains the prize, but we can only ask questions of the form \"Is the bin number in $S$?\" for some $S \\subseteq \\{1,2,3,4,5,6,7,8\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> a) With an optimal strategy, what is the expected number of questions we would need to ask, assuming that we get feedback after every question? Describe the sequence of questions we would ask, depending on what feedback we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> b) Let $X$ be a random variable for the number of the bin containing the ball. What is the entropy of $X$? (Use a logarithm of base 2.) How does this compare to the expected number of questions we asked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> c) Now consider the case where we have prior probabilities on how likely each bin is to contain the prize. Describe how we could use Huffman coding to find an efficient series of questions to ask, in order to figure out which bin contains the prize. (In fact, one can show that using Huffman coding helps you determine the optimal sequence of questions to ask.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> d) Let's look at a specific instance of this problem, where the bins have probabilities [0.4, 0.15, 0.12, 0.11, 0.07, 0.06, 0.05, 0.04] of containing the prize. Use your method HuffEncode from the previous question to calculate the expected number of questions you have to ask in order to determine which bin contains the prize, using this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> e) Repeat part b) for this new scenario, and compare your answer to the answer you obtained in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> f) Try a few more distributions, and compare the expected number of questions you need to ask with Huffman Coding to the entropy of the distribution, $H(X)$. Provide observed bounds for expected number of question with respect to $H(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
