{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "from collections import namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Abhi/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def add(self, state, action, next_state, reward):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append([])\n",
    "        self.buffer[self.index] = Transition(state, action, next_state, reward)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        ## TODO: create a new Transition tuple and add it to the buffer in the current index\n",
    "        # then increment the current index making sure to loop the index around when it reaches the capacity.\n",
    "        \n",
    "        \n",
    "    def get_sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_DECAY = 200\n",
    "EPS_END = 0.05\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "class DQN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(state_size, 32)\n",
    "        self.linear2 = torch.nn.Linear(32, 32)\n",
    "        self.output = torch.nn.Linear(32, 2)\n",
    "        self.steps = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## TODO: use linear1, linear2 and output to create a 3 layer net with relu's on each layer.\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.output(x))\n",
    "        return x\n",
    "        \n",
    "        #raise NotImplementedError('TODO')\n",
    "    \n",
    "    def get_action(self, state, _eval=False):\n",
    "        eps_thresh = EPS_END + (EPS_START - EPS_END) * np.exp(-1.0 * self.steps / EPS_DECAY)\n",
    "        self.steps += 1\n",
    "        if random.random() > eps_thresh or _eval:\n",
    "            with torch.no_grad():\n",
    "                # TODO: get the index of the max value of the output of the network\n",
    "                # HINT: look at pytorch's max function\n",
    "                index = torch.max(self.forward(state), 1)[1]\n",
    "                #print(index)\n",
    "                return index.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (linear1): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (output): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(net.parameters())\n",
    "replay_buffer = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    if len(replay_buffer.buffer) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    samples = replay_buffer.get_sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*samples))\n",
    "    \n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # TODO: get the values of the output of our net\n",
    "    state_batch_values = net(state_batch)\n",
    "    # TODO: from the above values choose only the ones that correspond to the actions in action_batch\n",
    "    # HINT: remember that for each sample in the batch their will be 2 output values one for each action\n",
    "    state_action_values = state_batch_values.gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # TODO: compute the expected state_action values from the next state values using the \n",
    "    # belman equation V(s_t) = V(s_{t+1}) * GAMMA + current_reward\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # TODO: Compute L1 loss between `state_action_values` and `expected_state_action_values`\n",
    "    #print(state_action_values, expected_state_action_values)\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in net.parameters():\n",
    "        # clamp gradients between -1 and 1\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:12<00:00, 77.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "num_episodes = 1000\n",
    "rewards = []\n",
    "for i_episode in tqdm(range(num_episodes)):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    state = torch.tensor(env.state, device=device, dtype=torch.float32).view(1, -1)\n",
    "    reward_sum = 0\n",
    "    for t in itertools.count():\n",
    "        # Select and perform an action\n",
    "        action = net.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        reward_sum += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        next_state = torch.tensor(next_state, device=device, dtype=torch.float32).view(1, -1)\n",
    "        # Store the transition in memory\n",
    "        replay_buffer.add(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        train_step()\n",
    "        if done:\n",
    "            rewards.append(reward_sum)\n",
    "            break\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.691\n",
      "18.0\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(rewards))\n",
    "print(np.max(rewards))\n",
    "print(len(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
